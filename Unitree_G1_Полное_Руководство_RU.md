# Гуманоидный робот Unitree G1: Полное руководство по управлению и программированию

Гуманоидным роботом Unitree G1 можно управлять через **VR-телеоперацию**, **обучение с подкреплением (RL)** и **имитационное обучение (IL)**. Unitree предоставляет обширный открытый инструментарий для всех подходов. Основная система телеоперации — **xr_teleoperate** (1.1k звёзд), которая поддерживает Apple Vision Pro, Meta Quest 3 и PICO 4 Ultra для управления в реальном времени со встроенным сбором данных для имитационного обучения. Для автономного управления **unitree_rl_gym** обеспечивает обучение на базе Isaac Gym с прямым переносом из симуляции в реальность, а **unitree_IL_lerobot** позволяет клонировать поведение с использованием архитектур ACT и Diffusion Policy.

G1 поставляется в конфигурациях от **23 до 43 степеней свободы (DOF)**, причём модель EDU Ultimate оснащена ловкими руками Dex3-1 (7 DOF на руку с 33 тактильными датчиками каждая). Вся разработка использует **unitree_sdk2** на базе CycloneDDS для низкоуровневого управления, с интеграцией ROS2 через **unitree_ros2**. Данное руководство объединяет основные статьи, репозитории и детали реализации, необходимые для работы с G1.

---

## VR-телеоперация — самый быстрый путь к управлению роботом

### xr_teleoperate: Официальный фреймворк телеоперации

**Репозиторий:** https://github.com/unitreerobotics/xr_teleoperate

Официальная система VR-телеоперации Unitree позволяет управлять гуманоидами G1 в реальном времени с помощью потребительских XR-гарнитур. Система передаёт стереоскопическое видео с камер робота на гарнитуру, одновременно отображая движения рук и кистей человека на робота через обратную кинематику и ретаргетинг рук.

**Поддерживаемые конфигурации оборудования:**
- **Гарнитуры:** Apple Vision Pro, Meta Quest 3, PICO 4 Ultra Enterprise
- **Роботы:** G1 (23 и 29 DOF), H1, H1_2
- **Манипуляторы:** ловкая рука Dex3-1, захват Dex1-1, рука Inspire, рука BrainCo
- **Режимы ввода:** отслеживание рук или контроллеров
- **Режимы отображения:** иммерсивный VR, эго (прозрачность + окно от первого лица), только прозрачность

Архитектура состоит из трёх основных компонентов: **teleimager** обрабатывает многокамерную трансляцию через ZeroMQ и WebRTC, **televuer** обеспечивает браузерную VR-визуализацию с использованием фреймворка Vuer, а **robot_control** реализует IK-решатель с dex-retargeting для отображения поз рук. Для установки требуется Python 3.10, Pinocchio 3.1.0 для кинематики и соответствующие SSL-сертификаты для связи с гарнитурой.

```bash
# Запуск телеоперации для G1 с руками Dex3-1
python teleop/main.py --arm=G1_29 --ee=dex3 --frequency=30 --display-mode=ego --record
```

Флаг `--record` включает встроенный сбор данных для имитационного обучения, сохраняя RGB-изображения, данные глубины, аудио и состояния робота в формате JSON, совместимом с конвейером обучения unitree_IL_lerobot.

### OpenTeleVision обеспечивает телеоперацию на межконтинентальных расстояниях

**Репозиторий:** https://github.com/OpenTeleVision/TeleVision | **Статья:** arXiv:2407.01512 (CoRL 2024)

OpenTeleVision продемонстрировал успешную телеоперацию на расстоянии **более 5000 км** с использованием стереоскопической визуальной обратной связи, транслируемой на VR-гарнитуры. Система интегрирует ACT (Action Chunking with Transformers) для имитационного обучения непосредственно из демонстраций телеоперации.

Ключевые технические инновации включают стереоскопическую трансляцию вида от первого лица для восприятия глубины, зеркалирование движений рук через оценку позы, а также поддержку как локальной (одна сеть), так и удалённой (через туннель ngrok) работы. Проект валидировал долгосрочные задачи, включая сортировку банок, вставку банок, складывание ткани и разгрузку посудомоечной машины на гуманоидных платформах.

### Требования к настройке для разных гарнитур

**Apple Vision Pro** требует SSL-сертификатов, сгенерированных mkcert и переданных на устройство через AirDrop, а также включения WebXR в настройках Safari. Гарнитура должна находиться в той же сети, что и хост-компьютер, с доступом к потоку по адресу `https://<host-ip>:8012`.

**Meta Quest 3** использует сетевую трансляцию через туннели ngrok, настраиваемую установкой `ngrok=True` в конструкторе OpenTeleVision. Этот подход работает между разными сетями, но добавляет задержку.

**PICO 4 Ultra Enterprise** предлагает лучшую производительность с поддержкой Wi-Fi 7, обеспечивающей **задержку трансляции ~5 мс**, кодек AV1 для эффективного сжатия и 32-мегапиксельные стереокамеры сквозного обзора. Фреймворк XRoboToolkit (arXiv:2508.00097) подтвердил **вставку отвёртки 3 мм с допуском 0.5 мм** благодаря точному отслеживанию PICO 4 Ultra.

### Оптимизация задержки достигает ~70 мс от начала до конца

Исследование из arXiv:2409.04639 достигло **задержки ~70 мс** от ввода VR до достижения роботом целевой позы с использованием бескалибровочного захвата движений всего с 7 IMU. Ключевые стратегии оптимизации включают настройки привязки к CPU для детерминированного тайминга, взвешенные скользящие фильтры для сглаживания суставов, интерполяцию движения во время переходов между состояниями и оптимизации фрагментного декодирования на устройствах PICO.

### Конвейеры сбора данных питают имитационное обучение

Система записи xr_teleoperate захватывает синхронизированные мультимодальные данные:
```
task_name/
├── episode_0001/
│   ├── audios/          # Аудиозаписи
│   ├── colors/          # RGB-изображения (640×480)
│   ├── depths/          # Изображения глубины
│   └── data.json        # Состояния, действия, временные метки
```

Эти данные напрямую конвертируются в формат LeRobot для обучения ACT, Diffusion Policy или других архитектур имитационного обучения. Система TWIST2 (arXiv:2511.02832) продемонстрировала сбор **100 демонстраций за 15 минут** с почти 100% успехом с использованием этого конвейера.

---

## Обучение с подкреплением для локомоции и управления всем телом

### unitree_rl_gym — официальный конвейер обучения RL

**Репозиторий:** https://github.com/unitreerobotics/unitree_rl_gym (2.5k звёзд)

Этот фреймворк на базе Isaac Gym реализует полный рабочий процесс от обучения до развёртывания в реальности: **Train → Play → Sim2Sim (MuJoCo) → Sim2Real**. Предобученные модели для локомоции G1 включены в `deploy/pre_train/g1/motion.pt`.

```bash
# Обучение политики локомоции G1
python legged_gym/scripts/train.py --task=g1 --headless

# Визуализация обученной политики
python legged_gym/scripts/play.py --task=g1

# Валидация sim-to-sim в MuJoCo
python deploy/deploy_mujoco/deploy_mujoco.py g1.yaml

# Развёртывание на реальном роботе
python deploy/deploy_real/deploy_real.py {net_interface} g1.yaml
```

Обучение обычно занимает **10 000 итераций** (~9 часов на RTX 4090) с 4096 параллельными средами. Фреймворк использует PPO с LSTM-сетями для временной информации, адаптивное планирование скорости обучения и комплексную рандомизацию домена.

### Ключевые гиперпараметры обучения и дизайн вознаграждений

Стандартные конфигурации используют **4096 параллельных сред**, 24 шага на среду и скорость обучения, начинающуюся с 1e-3 с адаптивным планированием. Рандомизация домена охватывает массу тела (±10-30%), трение (0.5-1.5), демпфирование суставов (±20%), толкающие силы (±100 Н) и задержки действий (0-2 шага).

Функция вознаграждения комбинирует отслеживание скорости (экспоненциальный штраф за ошибку команды), поддержание ориентации (проекция гравитации), плавность действий (штраф за временную разницу), штрафы за пределы суставов, периодические вознаграждения за походку (время ног в воздухе) и термины энергоэффективности. Эта формулировка производит стабильную, эффективную ходьбу, которая надёжно переносится на оборудование.

### Политики вставания обеспечивают восстановление после падений

**HumanUP** (arXiv:2502.12152) достиг **первой в мире демонстрации вставания гуманоида** на G1, восстанавливаясь как из положения на спине, так и на животе на плоской поверхности, траве, снегу и деформируемых поверхностях. Двухэтапный подход сначала обучает политику открытия с минимальными ограничениями для поиска жизнеспособных траекторий, затем обучает политику развёртывания с регуляризацией плавности и робастности.

### Среды симуляции охватывают несколько физических движков

**unitree_mujoco** (https://github.com/unitreerobotics/unitree_mujoco) предоставляет симуляцию MuJoCo с инструментами генерации ландшафта для валидации sim-to-sim перед реальным развёртыванием. G1 использует IDL `unitree_hg` для связи (отличается от `unitree_go`, используемого Go2/H1).

**unitree_rl_lab** (https://github.com/unitreerobotics/unitree_rl_lab) предлагает обучение на базе IsaacLab, совместимое с IsaacSim 5.1.0, поддерживая конфигурацию G1-29dof.

**Genesis** (https://genesis-embodied-ai.github.io/) обеспечивает обучение быстрее, чем IsaacGym, для крупномасштабных симуляций с унифицированной физикой и генеративным созданием миров. **HumanoidVerse** (https://github.com/LeCAR-Lab/HumanoidVerse) позволяет многосимуляторное обучение в IsaacGym, Genesis и IsaacLab.

### Критические статьи по переносу sim-to-real

| Статья | Ключевой вклад |
|--------|---------------|
| **ASAP** (arXiv:2502.01143) | Модель дельта-действий компенсирует несоответствие динамики; обеспечивает ловкие движения |
| **CLF-RL** (arXiv:2508.09354) | Руководство функцией Ляпунова; устойчивость к добавленной массе 3.55 кг |
| **Gait-Conditioned RL** (arXiv:2505.20619) | Единая политика нескольких походок для стояния/ходьбы/бега |

---

## Ловкая рука Dex3-1 обеспечивает сложную манипуляцию

### Спецификации оборудования и интерфейс управления

Рука Dex3-1 обеспечивает **7 DOF на руку** (3 большой палец + 2 указательный + 2 средний) с **33 тактильными датчиками давления** на руку, распределёнными по ладони и подушечкам пальцев. Связь работает на **1000 Гц** для обратной связи в реальном времени, поддерживая грузоподъёмность захвата до 500 г с повторяемостью кончика пальца ±2 мм.

Управление использует темы DDS через unitree_sdk2:
- Публикация: `rt/dex3/left/cmd`, `rt/dex3/right/cmd` (команды моторов)
- Подписка: `rt/dex3/left/state`, `rt/dex3/right/state` (7 состояний моторов + 9 датчиков давления)

SDK предоставляет позиционное управление (углы суставов через параметр `q`), силовое управление (гибридное с коэффициентами `kp`, `kd`) и прямой доступ к тактильной обратной связи для адаптации захвата.

### Исследования манипуляции и статьи по ловкому управлению

**GROOT N1** (arXiv:2503.14734) представляет модель Vision-Language-Action от NVIDIA с 2 миллиардами параметров для гуманоидной манипуляции, использующую двухсистемную архитектуру с Системой 2 (рассуждение VLM) для планирования задач и Системой 1 (диффузионный трансформер) для выполнения действий.

**ExBody** (arXiv:2402.16796) реализует экспрессивное управление всем телом с имитацией верхней части тела из человеческих демонстраций и отслеживанием скорости нижней части тела, развёрнуто на Unitree H1 и применимо к G1.

Для бимануальной координации **BiRP** (arXiv:2307.05933) изучает обобщённую бимануальную координацию с использованием относительной параметризации с гауссовскими смесями моделей, поддерживая как паттерны ведущий-ведомый, так и синергетическую координацию.

---

## Имитационное обучение: от демонстрации к развёртыванию

### unitree_IL_lerobot интегрируется с экосистемой LeRobot

**Репозиторий:** https://github.com/unitreerobotics/unitree_IL_lerobot

Этот модифицированный фреймворк LeRobot поддерживает обучение политик ACT, Diffusion Policy, Pi0 и GROOT на данных телеоперации G1. Конвейер конвертирует JSON-записи xr_teleoperate в формат LeRobot v3.0:

```bash
# Конвертация данных телеоперации в формат LeRobot
python unitree_lerobot/utils/convert_unitree_json_to_lerobot.py \
    --raw-dir $HOME/datasets \
    --repo-id your_name/repo_task_name \
    --robot_type Unitree_G1_Dex3

# Обучение политики ACT
python src/lerobot/scripts/lerobot_train.py \
    --dataset.repo_id=unitreerobotics/G1_Dex3_ToastedBread_Dataset \
    --policy.type=act

# Развёртывание на реальном роботе
python unitree_lerobot/eval_robot/eval_g1.py \
    --policy.path=path/to/checkpoint \
    --arm="G1_29" --ee="dex3" --send_real_robot=true
```

Предобученные датасеты доступны на Hugging Face: `unitreerobotics/G1_Dex3_ToastedBread_Dataset`.

### ACT достигает 80-90% успеха с 10 минутами демонстраций

**Статья:** arXiv:2304.13705 | **Проект:** https://tonyzhaozh.github.io/aloha/

Action Chunking with Transformers предсказывает **k действий одновременно** (обычно k=100) вместо одиночных действий, уменьшая накопление ошибок в k раз. Архитектура CVAE обрабатывает мультимодальное человеческое поведение, а временное ансамблирование сглаживает выполнение, агрегируя перекрывающиеся чанки действий.

Модель ~80M параметров обучается за несколько часов на одном GPU за 100k шагов, достигая скорости вывода 50-100 Гц. ACT рекомендуется как отправная точка для имитационного обучения G1 благодаря эффективности данных и быстрому обучению.

### Diffusion Policy обрабатывает сложное мультимодальное поведение

**Статья:** arXiv:2303.04137 | **Проект:** https://diffusion-policy.cs.columbia.edu/

Diffusion Policy генерирует действия через итеративное удаление шума, естественно обрабатывая разнообразные распределения действий и восстанавливаясь от возмущений. Для гуманоидных приложений **iDP3** (arXiv:2410.10803) адаптирует диффузионную политику с эгоцентрическими 3D-представлениями, устраняя требования калибровки камер и демонстрируя обобщение на невиданные среды из обучения на одной сцене.

### Ретаргетинг движений отображает человеческие демонстрации на действия робота

**GMR** (https://github.com/YanjieZe/GMR) обеспечивает общий ретаргетинг движений, поддерживающий **14+ гуманоидных роботов**, включая G1, с выполнением в реальном времени на CPU и множественными источниками ввода (OptiTrack, BVH, FBX, монокулярное видео через GVHMR).

**dex-retargeting** (https://github.com/dexsuite/dex-retargeting) реализует оптимизационное отображение поз рук, поддерживая как ретаргетинг телеоперации в реальном времени, так и офлайн постобработку для датасетов имитационного обучения.

---

## Инструменты разработки и интеграция SDK

### unitree_sdk2 обеспечивает низкоуровневое управление роботом

**C++ SDK:** https://github.com/unitreerobotics/unitree_sdk2 | **Python SDK:** https://github.com/unitreerobotics/unitree_sdk2_python

Оба SDK используют CycloneDDS для низколатентной связи. G1 использует тип сообщений `unitree_hg` (отличается от `unitree_go` для других роботов). Для совместимости с SDK требуется Python 3.10.

```bash
# Установка Python SDK
git clone https://github.com/eclipse-cyclonedds/cyclonedds -b releases/0.10.x
cd cyclonedds && mkdir build install && cd build
cmake .. -DCMAKE_INSTALL_PREFIX=../install && cmake --build . --target install

export CYCLONEDDS_HOME="~/cyclonedds/install"
pip3 install -e unitree_sdk2_python
```

Ключевые примеры в SDK: `/example/g1/` для специфичного управления локомоцией G1, `/example/low_level/` для управления на уровне суставов и `g1_dex3_example.cpp` для управления ловкой рукой.

### Интеграция ROS2 через unitree_ros2

**Репозиторий:** https://github.com/unitreerobotics/unitree_ros2

Пакет предоставляет темы ROS2 для состояния спортивного режима, низкоуровневого состояния и низкоуровневых команд. Специфичные для G1 примеры включают `g1_low_level_example` и `read_low_state_hg`.

**G1Pilot** (https://github.com/hucebot/g1pilot) предлагает разработанный сообществом пакет ROS2 с режимами управления по суставам/декартовым координатам, обратной связью IMU и одометрии, телеметрией каждого мотора (температура, напряжение) и развёртыванием на базе Docker на Ubuntu 22.04 + ROS2 Humble.

### Конфигурация сети и доступ к роботу

G1 использует подсеть **192.168.123.0/24** с компьютером разработки по адресу **192.168.123.164** (учётные данные: unitree/123) и компьютером локомоции по адресу 192.168.123.161. Подключитесь через Ethernet к порту на задней части шеи G1 и настройте статический IP на вашей машине разработки.

```bash
# Настройка внешнего компьютера
sudo ip addr add 192.168.123.199/24 dev enp2s0
sudo ip link set enp2s0 up
ssh unitree@192.168.123.164  # Пароль: 123
```

### Файлы модели робота для симуляции

- **Официальный URDF:** https://github.com/unitreerobotics/unitree_ros/tree/master/robots/g1_description
- **MuJoCo Menagerie:** https://github.com/google-deepmind/mujoco_menagerie/tree/main/unitree_g1 (включает `g1.xml` и `g1_with_hands.xml`)
- **Пакет robot_descriptions:** `pip install robot_descriptions`, затем `load_robot_description("g1_mj_description")`

---

## Иерархическое управление и интеграция моделей vision-language

### VLM обеспечивают высокоуровневое планирование задач

**VLM-TAMP** (arXiv:2410.02193) комбинирует высокоуровневое планирование VLM с планированием задач и движения, достигая 50-100% успеха в долгосрочных кухонных задачах, требующих 30-50 действий. VLM предлагает последовательности подцелей, а TAMP обрабатывает геометрическую осуществимость.

**GROOT N1** реализует двухсистемную архитектуру, где Система 2 (VLM) выполняет рассуждения о задачах, а Система 1 (диффузионная модель) выполняет низкоуровневые действия. Этот паттерн разделяет высокочастотное управление от более медленного совещательного планирования.

### Многоуровневые архитектуры управления

Трёхуровневый иерархический фреймворк (arXiv:2409.08488) разделяет управление на: верхний уровень (точная модель динамики через глубокую остаточную сеть), средний уровень (политики стабильности и координации) и нижний уровень (высокочастотное управление суставами). Это позволяет использовать MPC всего тела даже с медленными обновлениями политики верхнего уровня.

**HumanoidBench** (https://github.com/carlosferrazza/humanoid-bench) предоставляет 27 задач управления всем телом для бенчмаркинга иерархических подходов с базовыми линиями, комбинирующими низкоуровневые политики навыков (ходьба, достижение), оркестрируемые высокоуровневыми планировщиками.

### Обучение навыкам использует примитивы движения

Динамические примитивы движения (DMP) и вероятностные примитивы движения (ProMP) кодируют переиспользуемые паттерны движения. Библиотека **movement_primitives** (https://github.com/dfki-ric/movement_primitives) поддерживает декартовы движения, бимануальную манипуляцию с терминами связи и обусловливание на промежуточных точках.

**SkillBlender** (arXiv:2506.09366) обеспечивает локо-манипуляцию всего тела путём смешивания нескольких примитивов навыков, позволяя плавные переходы между поведениями локомоции и манипуляции.

---

## Развёртывание в реальном мире и вопросы безопасности

### Лучшие практики переноса sim-to-real

**Рандомизация домена** остаётся основной техникой, с CDR (arXiv:2403.12193), показывающим улучшенный перенос через последовательное обучение на подмножествах параметров. Ключевые параметры для рандомизации: масса (±10-30%), трение (0.5-1.5×), задержки действий (0-2 шага), шум датчиков (±5%) и толкающие силы (±100 Н).

**Дистилляция учитель-ученик** в Isaac Lab обучает учителя с привилегированными наблюдениями (например, скорость корня), дистиллирует в ученика через клонирование поведения, затем дообучает с RL, используя только наблюдения реальных датчиков. Финальная политика экспортируется как .pt/.onnx для развёртывания.

**TRANSIC** (ICRA 2024) добавляет коррекции человека в цикл через базовую политику (обученную в симуляции) плюс остаточную политику (коррекции реального мира), эффективно для манипуляции с богатыми контактами, как сборка мебели.

### Фреймворки безопасности и стандарты

**Фреймворк SPARK** (https://github.com/intelligent-control-lab/spark, arXiv:2502.02858) реализует проецируемые алгоритмы безопасного множества для многоограничительной ловкой безопасности, валидированные на G1 для избегания столкновений. Система обрабатывает геометрические ограничения на уровне конечностей в реальном времени.

Рабочие группы IEEE и ISO разрабатывают специфичные для гуманоидов стандарты безопасности (ожидается 18-36 месяцев до ратификации), адресуя уникальную проблему, что выключение означает падение для динамически сбалансированных роботов.

### Чек-лист развёртывания для G1

1. Войдите в режим отладки через пульт (L2+R2) перед разработкой SDK
2. Отключите встроенный sport_mode через приложение, чтобы предотвратить конфликты инструкций
3. Настройте сетевое подключение к подсети 192.168.123.x
4. Выполните калибровку суставов согласно руководству для конкретной модели
5. Тестируйте в симуляции (unitree_mujoco) перед реальным развёртыванием
6. Реализуйте проверки безопасного завершения: ориентация (>1.0 рад), скорость сустава (>10 рад/с), температура мотора (>120°C обмотка, >85°C корпус), батарея (<20%)
7. Поддерживайте безопасную дистанцию; начинайте с медленных движений и низких моментов

---

## Сводка основных ресурсов

### Основные репозитории по функциям

| Функция | Репозиторий | Звёзды |
|---------|-------------|--------|
| VR-телеоперация | unitreerobotics/xr_teleoperate | 1.1k |
| Обучение RL | unitreerobotics/unitree_rl_gym | 2.5k |
| Имитационное обучение | unitreerobotics/unitree_IL_lerobot | 479 |
| C++ SDK | unitreerobotics/unitree_sdk2 | 758 |
| Python SDK | unitreerobotics/unitree_sdk2_python | 486 |
| Симуляция MuJoCo | unitreerobotics/unitree_mujoco | 707 |
| Интеграция ROS2 | unitreerobotics/unitree_ros2 | — |
| Симуляция Isaac Lab | unitreerobotics/unitree_sim_isaaclab | — |

### Ключевые исследовательские статьи

- **OpenTeleVision** (arXiv:2407.01512): Иммерсивная телеоперация со стереоскопической обратной связью
- **HumanUP** (arXiv:2502.12152): Первое в мире вставание гуманоида
- **ASAP** (arXiv:2502.01143): Модель дельта-действий для ловкого sim-to-real
- **ACT** (arXiv:2304.13705): Чанкинг действий для эффективного имитационного обучения
- **Diffusion Policy** (arXiv:2303.04137): Визуомоторное обучение через диффузию действий
- **iDP3** (arXiv:2410.10803): 3D диффузионная политика для гуманоидной манипуляции
- **GROOT N1** (arXiv:2503.14734): Фундаментальная модель vision-language-action
- **HOMIE** (arXiv:2502.13013): Локо-манипуляция с экзоскелетом
- **TWIST** (arXiv:2505.02833): Система полного телеуправления
- **Clone** (arXiv:2506.08931): Телеоперация с замкнутым контуром

### Официальная документация

- **Руководство разработчика G1:** https://support.unitree.com/home/en/G1_developer
- **Руководство по телеоперации:** https://support.unitree.com/home/zh/Teleoperation
- **Предобученные модели:** https://huggingface.co/unitreerobotics

---

## Заключение

Unitree G1 выигрывает от необычно полной открытой экосистемы, охватывающей телеоперацию, обучение с подкреплением и имитационное обучение. Для быстрого прототипирования **xr_teleoperate с Apple Vision Pro или Meta Quest 3** обеспечивает самый быстрый путь к управлению роботом со встроенным сбором данных для последующего обучения политик. Для автономного поведения конвейер **unitree_rl_gym → валидация MuJoCo → sim2real** предлагает проверенный перенос локомоции, а **unitree_IL_lerobot с ACT** позволяет изучать навыки манипуляции с 20-50 демонстраций.

Критическая точка интеграции — коммуникационный слой **unitree_sdk2** на базе CycloneDDS — все высокоуровневые фреймворки в конечном счёте взаимодействуют через этот SDK. Разработчикам следует начинать с предобученных моделей и примеров кода, валидировать в симуляции MuJoCo, затем развёртывать инкрементально с включёнными проверками безопасного завершения. Комбинация VR-телеоперации для сбора данных и имитационного обучения на базе трансформеров (ACT, Diffusion Policy) представляет современный уровень развития для обучения G1 новым навыкам манипуляции.

---

## Быстрый старт: Рекомендуемый путь обучения

### Неделя 1-2: Базовое понимание
1. Изучите официальную документацию: https://support.unitree.com/home/en/G1_developer
2. Установите unitree_sdk2_python и запустите базовые примеры
3. Настройте сетевое подключение к роботу

### Неделя 3-4: Симуляция
1. Установите unitree_mujoco и Isaac Gym
2. Запустите предобученные модели в симуляции
3. Изучите структуру unitree_rl_gym

### Неделя 5-6: Телеоперация
1. Настройте xr_teleoperate с вашей VR-гарнитурой
2. Практикуйтесь в телеоперации базовых движений
3. Начните сбор данных для имитационного обучения

### Неделя 7-8: Имитационное обучение
1. Установите unitree_IL_lerobot
2. Обучите первую политику ACT на собранных данных
3. Разверните политику на реальном роботе

### Неделя 9+: Продвинутые темы
1. Исследуйте Diffusion Policy для сложных задач
2. Интегрируйте VLM для высокоуровневого планирования
3. Разработайте собственные навыки и задачи
